# Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning

<!-- ![Method Image](./data/readme/method-fig.png) -->

[![Paper](https://img.shields.io/badge/Paper-arXiv%20preprint-b31b1b.svg)](https://arxiv.org/abs/2504.05108)
[![License](https://img.shields.io/github/license/CLAIRE-Labo/EvoTune)](./LICENSE)




## Overview

**EvoTune** is a framework for discovering new algorithms by combining:

1. Evolutionary search over LLM-generated Python programs, and
2. Reinforcement Learning to fine-tune the search operator - the LLM - based on performance scores of discovered algorithms .



## Repo Structure

The core codebase lives under ```src/``` and is organized as follows:

```plaintext

evotune/
â”œâ”€â”€ configs/                  # Hydra-based config system
â”‚   â”œâ”€â”€ accelerate_config/    # Accelerate configs
â”‚   â”œâ”€â”€ cluster/              # SLURM / cluster overrides
â”‚   â”œâ”€â”€ model/                # Model-specific settings
â”‚   â”œâ”€â”€ sweep/                # Sweep configuration files
â”‚   â”œâ”€â”€ task/                 # Per-task configs (e.g., bin, tsp, etc.)
â”‚   â”œâ”€â”€ train/                # Training configuration
â”‚   â””â”€â”€ config.yaml           # Default config
â”œâ”€â”€ data/                     # TSP and flatpack datasets
â”œâ”€â”€ installation/             # Dockerfiles for various hardware
â”œâ”€â”€ scripts/                  # Example launch scripts for sweeps
â”‚   â”œâ”€â”€ run_eval_sweep_example.sh
â”‚   â””â”€â”€ run_train_sweep_example.sh
â”œâ”€â”€ src/
|   â”œâ”€â”€ packing/              # Core EvoTune framework
|   â”‚   â”œâ”€â”€ evaluate/         # Task-specific logic (registered via registry)
|   â”‚   â”‚   â”œâ”€â”€ bin_packing/
|   â”‚   â”‚   â”œâ”€â”€ flat_pack/
|   â”‚   â”‚   â”œâ”€â”€ tsp/
|   â”‚   â”‚   â”œâ”€â”€ registry.py   # Task registry
|   â”‚   â”‚   â””â”€â”€ README.md     # How to add new tasks
|   â”‚   â”œâ”€â”€ funsearch/        # Program database implementation
|   â”‚   â”œâ”€â”€ logging/          # Logging, statistics, and function tracking
|   â”‚   â”œâ”€â”€ model/            # Prompting, LLM I/O, inference engine setup
|   â”‚   â”œâ”€â”€ parallel/         # Multiprocessing producers & consumers
|   â”‚   â”œâ”€â”€ train/            # DPO pipelines for fine-tuning LLMs
|   â”‚   â””â”€â”€ utils/            # Seeding, function helpers, etc.
|   â””â”€â”€  experiments/         # Scripts for specific experiments (train / eval)
â”œâ”€â”€ pyproject.toml
â””â”€â”€ LICENSE
```



## Setup & Dependencies

To create the Python environment for running experiments, use one of the provided **Dockerfiles** that matches your machine architecture and desired inference backend:

```plaintext
installation/
â”œâ”€â”€ docker-amd64-cuda-tgi/   # For x86_64 machines using TGI
â”œâ”€â”€ docker-amd64-cuda-vllm/  # For x86_64 machines using vLLM
â””â”€â”€ docker-arm64-cuda/       # For ARM64 machines using vLLM
```

> Most experiments for the paper were run using **A100 GPUs (80GB)**.


## How to Run the Code

### Single Runs

The two main entry points are located in:

```plaintext
src/experiments/
â”œâ”€â”€ main.py   # For running training with evolution + finetuning
â”œâ”€â”€ eval.py   # For evaluating saved programbanks
```

### Sweep Runs

We provide example sweep scripts in the ```scripts/``` folder:

```plaintext
scripts/
â”œâ”€â”€ run_eval_sweep_example.sh
â”œâ”€â”€ run_train_sweep_example.sh
```

These are designed to be used with job schedulers like SLURM or RunAI. To use them:

1. Fill in the ```# TODO``` block in each script with your cluster submission logic.
2. Configure the sweep/grid settings in the appropriate ```configs/sweep/``` and ```configs/cluster/``` YAML files.
3. Launch your sweep using the modified script.

> You can also run sweeps locally by adapting these scripts, just remove the SLURM logic.

### Notes 
As the project evolved, so did the code. We are open-sourcing the latest version as it is easier to work with after a round of refactoring and other minor updates (for example, improved extraction of functions from LLM outputs). These changes may introduce small discrepancies in the results. In the paper, the bin packing and traveling salesman problem results were generated with the TGI inference engine, whereas the Flatpack, Hash Code, and LLM-SR experiments used vLLM. We added vLLM support to simplify running the code on clusters with ARM64 architecture.


## Adding a New Task

To add your own task:

ðŸ‘‰ Navigate to:

```src/packing/evaluate/README.md```

Youâ€™ll find instructions for implementing and registering a new task with following components:

- ```generate_input```
- ```evaluate_func```
- ```get_initial_func```
- ```system_prompt``` / ```append_prompt```


### Citation
```bibtex
@inproceedings{surina2025algorithm,
title={Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning},
author={Anja Surina and Amin Mansouri and Lars C.P.M. Quaedvlieg and Amal Seddas and Maryna Viazovska and Emmanuel Abbe and Caglar Gulcehre},
booktitle={Second Conference on Language Modeling},
year={2025},
}
```